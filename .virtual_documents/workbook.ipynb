import numpy as np
import tensorflow as tf
from tensorflow import keras
import time
import pickle
from collections import deque
from pingpong import pingpongRL
import matplotlib.pyplot as plt


alpha=0.01
alpha_decay=0.01
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(24, input_dim=5, activation='tanh')) # input the 5 dimensions of the state
model.add(tf.keras.layers.Dense(48, activation='tanh'))
model.add(tf.keras.layers.Dense(2, activation='linear')) # output the Q-values (value function) for the two actions

optimizer=keras.optimizers.Adam(learning_rate=alpha, decay=alpha_decay)
loss = keras.losses.MeanSquaredError()
model.compile(optimizer=optimizer, loss=loss)


state = np.array([[1,1,1,1,0]])


start = time.time()
np.argmax(model.predict(state))
print(time.time()-start)


PATH = "RL/agents/"
time = 1036
mem = pickle.load(open(f"{PATH}mem{210624}_{time}.p", "rb"))
loss = pickle.load(open(f"{PATH}loss{210624}_{time}.p", "rb"))
score = pickle.load(open(f"{PATH}scores{210624}_{time}.p", "rb"))


fig, axs = plt.subplots(2,1)
episodes = np.linspace(0,len(loss)-1, len(loss))
ax=axs[0]
ax.plot(episodes, loss)
ax=axs[1]
ax.plot(episodes, score)


episode = [mem[0]]


episode = []
for m in mem:
    episode.append(m[0][0])
    if m[4]:
        break


game = pingpongRL.Game()


game.playback(episode)



